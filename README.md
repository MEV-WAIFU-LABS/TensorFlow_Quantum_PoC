## quantum computing tensorflow on kubernetes

<br>

#### ðŸ‘‰ this repository contains my work deploying a quantum computing version of tensorflow in kubernetes
#### ðŸ‘‰ it leverages [kubeflow](https://www.kubeflow.org/) (a machine learning toolkit for kubernetes); [here is the config file](kubeflow)

<br>

-----

### pre-requisites

<br>

* **[vagrant](https://www.vagrantup.com/)**
* **[virtual box](https://www.virtualbox.org/)**
* **[kubeflow and minikf](https://www.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/)**
* **[tensorflow quantum](https://github.com/tensorflow/quantum/blob/master/docs/install.md)**

<br>

---

### theoretical background

<br>

* with the recent progress in the development of quantum computing, the development of new quantum ML models could have a profound impact on the worldâ€™s biggest problems, leading to breakthroughs in the areas of medicine, materials, sensing, and communications

* tensorflow quantum (TFQ) provides the tools for bringing quantum computing and machine learning research together to control and model natural or artificial quantum systems; e.g. (NISQ) processors with ~50 - 100 qubits

* TFQ integrates Cirq with TensorFlow, and offers high-level abstractions for the design and implementation of both discriminative and generative quantum-classical models by providing quantum computing primitives compatible with existing TensorFlow APIs, along with high-performance quantum circuit simulators

* a quantum model can represent and generalize data with a quantum mechanical origin:
    - quantum data exhibits superposition and entanglement, leading to joint probability distributions that could require an exponential amount of classical computational resources to represent or store
    - quantum data generated by NISQ processors are noisy and typically entangled just before the measurement occurs. however, applying quantum machine learning to noisy entangled quantum data can maximize extraction of useful classical information

* TFQ contains the basic structures, such as qubits, gates, circuits, and measurement operators that are required for specifying quantum computations. user-specified quantum computations can then be executed in simulation or on real hardware

* TFQ allows researchers to construct quantum datasets, quantum models, and classical control parameters as tensors in a single computational graph. the outcome of quantum measurements, leading to classical probabilistic events, is obtained by TensorFlow Ops (training can be done using standard Keras functions)

* a key feature of TensorFlow Quantum is the ability to simultaneously train and execute many quantum circuits. this is achieved by TensorFlowâ€™s ability to parallelize computation across a cluster of computers, and the ability to simulate relatively large quantum circuits on multi-core computers

<br>

--- 

### steps to run this experiment

<br>

#### 1. prepare a quantum dataset

- quantum data is loaded as tensors (a multi-dimensional array of numbers)
- each quantum data tensor is specified as a quantum circuit written in Cirq that generates quantum data on the fly 
- the tensor is executed by TensorFlow on the quantum computer to generate a quantum dataset

<br>

#### 2. evaluate a quantum neural network model 

- the researcher can prototype a quantum neural network using Cirq that they will later embed inside of a TensorFlow compute graph 
- parameterized quantum models can be selected from several broad categories based on knowledge of the quantum data's structure 
- the goal of the model is to perform quantum processing in order to extract information hidden in a typically entangled state 

<br>

#### 3. sample or average 

- measurement of quantum states extracts classical information in the form of samples from a classical random variable
- the distribution of values from this random variable generally depends on the quantum state itself and the measured observable
- as many variational algorithms depend on mean values of measurements, also known as expectation values, TFQ provides methods for averaging over several runs involving steps (1) and (2)

<br>

#### 4. evaluate a classical neural networks model 

- once classical information has been extracted, it is in a format amenable to further classical post-processing
- as the extracted information may still be encoded in classical correlations between measured expectations, classical deep neural networks can be applied to distill such correlations

<br>

#### 5. evaluate the cost function 

- given the results of classical post-processing, a cost function is evaluated 
- this could be based on how accurately the model performs the classification task if the quantum data was labeled, or other criteria if the task is unsupervised

<br>

#### 6. evaluate gradients & update parameters 

- after evaluating the cost function, the free parameters in the pipeline should be updated in a direction expected to decrease the cost
- this is most commonly performed via gradient descent




